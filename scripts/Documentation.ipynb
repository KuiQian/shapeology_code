{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mController.py\u001b[m\u001b[m\r\n",
      "Create sequence of VQs.py\r\n",
      "Documentation.ipynb\r\n",
      "MasterProcessor.py\r\n",
      "diffusion_maps.py\r\n",
      "\u001b[31mextractPatches.py\u001b[m\u001b[m\r\n",
      "files\r\n",
      "\u001b[31mrun_job.py\u001b[m\u001b[m\r\n",
      "\u001b[31mtest.py\u001b[m\u001b[m\r\n",
      "\u001b[31mwatchdog.py\u001b[m\u001b[m\r\n",
      "\u001b[31mwatchdog.sh\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MasterProcessor.py\n",
    "defunct, replaced with \"Controller\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controller.py\n",
    "\n",
    "Top level controller of processing\n",
    "\n",
    "```\n",
    "usage: Controller.py [-h] scripts s3location local_data\n",
    "\n",
    "positional arguments:\n",
    "  scripts     path to the directory with the scripts\n",
    "  s3location  path to the s3 directory with the lossless images\n",
    "  local_data  path to the local data directory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import psutil\n",
    "import socket\n",
    "from os import getpid,mkdir,system\n",
    "from subprocess import Popen,PIPE\n",
    "from os.path import isfile\n",
    "from glob import glob\n",
    "from time import sleep,time\n",
    "from os.path import isfile\n",
    "from sys import argv\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def run(command):\n",
    "    print('cmd=',command)\n",
    "    system(command)\n",
    "    \n",
    "def runPipe(command):\n",
    "    print('runPipe cmd=',command)\n",
    "    p=Popen(command.split(),stdout=PIPE,stderr=PIPE)\n",
    "    L=p.communicate()\n",
    "    stdout=L[0].decode(\"utf-8\").split('\\n')\n",
    "    stderr=L[1].decode(\"utf-8\").split('\\n')\n",
    "    return stdout,stderr\n",
    "\n",
    "def clock(message):\n",
    "    print('%8.1f \\t%s'%(time(),message))\n",
    "    time_log.append((time(),message))\n",
    "\n",
    "def printClock():\n",
    "    t=time_log[0][0]\n",
    "    for i in range(1,len(time_log)):\n",
    "        print('%8.1f \\t%s'%(time_log[i][0]-t,time_log[i][1]))\n",
    "        t=time_log[i][0]\n",
    "\n",
    "def list_s3_files(stack_directory):\n",
    "    stdout,stderr=runPipe(\"aws s3 ls %s/ \"%(stack_directory))\n",
    "    return stdout\n",
    "    \n",
    "def get_file_table(stack_directory,pattern=r'(.*)\\.([^\\.]*)$'):\n",
    "    \"\"\"create a table of the files in a directory corresponding to a stack:\n",
    "    stack_directory: the location of the directory on s3.\n",
    "    example: s3://mousebraindata-open/MD657/\n",
    "    \"\"\"\n",
    "\n",
    "    stdout = list_s3_files(stack_directory)\n",
    "    pat=re.compile(pattern)\n",
    "\n",
    "    T={}\n",
    "    for file in stdout:\n",
    "        parts=file.strip().split()\n",
    "        if len(parts)!=4:\n",
    "            continue\n",
    "        filename=parts[3]\n",
    "        if not 'lossless.' in filename:\n",
    "            continue\n",
    "        m=pat.match(filename)\n",
    "        if m:\n",
    "            file,ext= m.groups()\n",
    "            info=(ext,parts[0]+' '+parts[1])\n",
    "            if file in T:\n",
    "                T[file].append(info)\n",
    "            else:\n",
    "                T[file]=[info]\n",
    "        else:\n",
    "            print(filname,'no match')\n",
    "    return T\n",
    "\n",
    "def find_and_lock(stack_directory):\n",
    "    \"\"\" find a section file without a lock and lock it\"\"\"\n",
    "    T=get_file_table(stack_directory)\n",
    "\n",
    "    while True:\n",
    "        #find a file without a lock\n",
    "        found=False\n",
    "        for item in T.items():\n",
    "            if len(item[1])==1:\n",
    "                found=True\n",
    "                break\n",
    "        if not found:\n",
    "            return None\n",
    "        \n",
    "        filename=item[0]\n",
    "        extensions=item[1]\n",
    "\n",
    "        #create a lock\n",
    "        hostname=socket.gethostname().replace('.','+')\n",
    "        flagname=filename+'.lock-'+hostname\n",
    "        open(scripts+'/'+flagname,'w').write(flagname+'\\n')\n",
    "\n",
    "        command='aws s3 cp %s %s/%s'%(scripts+'/'+flagname,stack_directory,flagname)\n",
    "        run(command)\n",
    "\n",
    "        # check to make sure that there is only one lock.\n",
    "        T=get_file_table(stack_directory)\n",
    "        extensions=T[filename]\n",
    "        if len(extensions)==2:\n",
    "            return filename\n",
    "    \n",
    "        # translation of date for better handling of two machines putting locks \n",
    "        # at nearly the same time\n",
    "        # comparing the time stamps of the locks can be used to resolve who was firstand should continue\n",
    "        # and who should look for another file.\n",
    "        # from datetime import datetime\n",
    "        # d1=datetime.strptime('2018-08-28 21:16:34','%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def process_tiles(tile_pattern):\n",
    "    i=0\n",
    "    print('tile_pattern=',tile_pattern)\n",
    "    for infile in glob(tile_pattern):\n",
    "        stem=infile[:-4]\n",
    "        #print ('infile=%s, stem=%s'%(infile,stem))\n",
    "        lockfile=stem+'.lock'\n",
    "        if not isfile(lockfile):\n",
    "            i+=1\n",
    "            print('got lock',lockfile,i)\n",
    "            run('python3 {0}/run_job.py {0} {1}'.format(scripts,stem))\n",
    "            sleep(0.1)\n",
    "        else:\n",
    "            #print('\\r %s exists'%lockfile,end='')\n",
    "            continue\n",
    "\n",
    "        # Wait if load is too high\n",
    "        load=np.mean(psutil.cpu_percent(percpu=True))\n",
    "        print(' %5d                            load: %6.2f'%(i,load))\n",
    "        j=0\n",
    "        while load>85:\n",
    "            print(' %5d    Sleep:%3d               load: %6.2f'%(i,j,load))\n",
    "            j+=1\n",
    "            sleep(2)\n",
    "            load=np.mean(psutil.cpu_percent(percpu=True))\n",
    "\n",
    "        print('\\nload low enough',load)\n",
    "    return i\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"scripts\", type=str,\n",
    "                        help=\"path to the directory with the scripts\")\n",
    "    parser.add_argument(\"s3location\", type=str,\n",
    "                        help=\"path to the s3 directory with the lossless images\")\n",
    "    parser.add_argument(\"local_data\",type=str,\n",
    "                        help=\"path to the local data directory\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    scripts=args.scripts\n",
    "    stack_directory=args.s3location\n",
    "    local_data=args.local_data\n",
    "    \n",
    "    time_log=[]\n",
    "\n",
    "    clock('starting Controller with stack_directory=%s, local_data=%s'%(stack_directory,local_data))\n",
    "\n",
    "    try:\n",
    "        #preparations: make dirs data and data/tiles\n",
    "        run('sudo chmod 0777 /dev/shm/')\n",
    "        mkdir(local_data)\n",
    "        mkdir(local_data+'/tiles')\n",
    "        clock('created data directory')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    while True:\n",
    "        #find an unprocessed file on S3\n",
    "        stem=find_and_lock(stack_directory)\n",
    "        clock('found and locked %s'%stem)\n",
    "\n",
    "        if stem==None:\n",
    "            print('all files processed')\n",
    "            break\n",
    "\n",
    "        run('rm -rf %s/*'%(local_data))\n",
    "        run('mkdir %s/tiles'%local_data)\n",
    "        clock('cleaning local directory')\n",
    "\n",
    "\n",
    "        #Bring in a file and break it into tiles\n",
    "        run('aws s3 cp %s/%s.jp2 %s/%s.jp2'%(stack_directory,stem,local_data,stem))\n",
    "        clock('copied from s3: %s'%stem)\n",
    "        run('kdu_expand -i %s/%s.jp2 -o %s/%s.tif'%(local_data,stem,local_data,stem))\n",
    "        clock('translated into tif')\n",
    "        run('convert %s/%s.tif -crop 1000x1000  +repage  +adjoin  %s'%\n",
    "            (local_data,stem,local_data)+'/tiles/tiles_%02d.tif')\n",
    "        clock('broke into tiles')\n",
    "\n",
    "        # perform analysis\n",
    "        i=process_tiles('%s/tiles/tiles_*.tif'%local_data)\n",
    "        clock('1 - processed %6d tiles'%i)\n",
    "        i=process_tiles('%s/tiles/tiles_*.tif'%local_data)\n",
    "        clock('2 - processed %6d tiles'%i)\n",
    "\n",
    "        #copy results to s3\n",
    "        run(\"tar czf {0}/{1}_patches.tgz {0}/tiles/*.pkl {0}/tiles/*.log {0}/tiles/*.lock\".format(local_data,stem))\n",
    "        clock('created tar file {0}/{1}_patches.tgz'.format(local_data,stem))\n",
    "\n",
    "        run('aws s3 cp {0}/{1}_patches.tgz {2}/'.format(local_data,stem,stack_directory))\n",
    "        clock('copy tar file to S3')\n",
    "\n",
    "    printClock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load run_job.py\n",
    "from sys import argv\n",
    "from os import getpid,system\n",
    "from os.path import isfile\n",
    "from time import sleep\n",
    "import datetime\n",
    "\n",
    "\n",
    "def getLock(lockfile):\n",
    "    try:\n",
    "        l=open(lockfile, 'x')\n",
    "        return l\n",
    "    except FileExistsError:\n",
    "        return None\n",
    "\n",
    "scripts=argv[1]\n",
    "stem=argv[2]\n",
    "\n",
    "lockfilename=stem+'.lock'\n",
    "logfilename=stem+'.log'\n",
    "\n",
    "lockfile=getLock(lockfilename)\n",
    "if not lockfile is None:\n",
    "\n",
    "    command='python3 %s/extractPatches.py %s > %s &'%(scripts,stem,logfilename)\n",
    "    # put some info into the log/lock file\n",
    "    now = datetime.datetime.now()\n",
    "    print('pid=',getpid(), file=lockfile)\n",
    "    print('start time=',now.isoformat(), file=lockfile)\n",
    "    print('command=',command, file=lockfile)\n",
    "    \n",
    "    system(command)\n",
    "    sleep(1)\n",
    "else:\n",
    "    print(lockfilename,'exists, skipping')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load watchdog.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from os.path import isfile,getmtime\n",
    "from glob import glob\n",
    "from time import sleep,time\n",
    "from os import system\n",
    "from subprocess import Popen,PIPE\n",
    "\n",
    "stack='s3://mousebraindata-open/MD657'\n",
    "local_data='/dev/shm/data'\n",
    "exec_dir='/home/ubuntu/shapeology_code/scripts'\n",
    "\n",
    "def runPipe(command):\n",
    "    print('cmd=',command)\n",
    "    p=Popen(command.split(),stdout=PIPE,stderr=PIPE)\n",
    "    L=p.communicate()\n",
    "    stdout=L[0].decode(\"utf-8\").split('\\n')\n",
    "    stderr=L[1].decode(\"utf-8\").split('\\n')\n",
    "    return stdout,stderr\n",
    "\n",
    "def run(command,out):\n",
    "    print('cmd=',command,'out=',out)\n",
    "    outfile=open(out,'w')\n",
    "    Popen(command.split(),stdout=outfile,stderr=outfile)\n",
    "\n",
    "def Last_Modified(file_name):\n",
    "    try:\n",
    "        mtime = getmtime(file_name)\n",
    "    except OSError:\n",
    "        mtime = 0\n",
    "    return(mtime)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    Recent=False\n",
    "    for logfile in glob(exec_dir+'/Controller*.log'):\n",
    "        gap=time() - Last_Modified(logfile)\n",
    "        if gap <120: # allow 2 minute idle\n",
    "            print(logfile,'gap is %6.1f'%gap)\n",
    "            Recent=True\n",
    "            break\n",
    "    if(not Recent):\n",
    "        # Check that another 'controller' is not running\n",
    "        stdout,stderr = runPipe('ps aux')\n",
    "        Other_controller=False\n",
    "        for line in stdout:\n",
    "            if 'Controller.py' in line:\n",
    "                Other_controller=True\n",
    "                break\n",
    "        \n",
    "        if Other_controller:\n",
    "            print('Other Controller.py is running')\n",
    "        else:\n",
    "            command='{0}/Controller.py {0} {1} {2}'\\\n",
    "                .format(exec_dir,stack,local_data)\n",
    "            output='{0}/Controller-{1}.log'.format(exec_dir,int(time()))\n",
    "            run(command,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load watchdog.sh\n",
    "#!/usr/bin/env bash\n",
    "export PATH=\"/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/KDU7A2_Demo_Apps_for_Centos7-x86-64_170827/:/home/ubuntu/shapeology_code/scripts/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\n",
    "export LD_LIBRARY_PATH=/home/ubuntu/KDU7A2_Demo_Apps_for_Centos7-x86-64_170827/\n",
    "#echo 'this is watchdog'  >> /home/ubuntu/watchdog.log\n",
    "#echo $PATH  >> /home/ubuntu/watchdog.log\n",
    "echo $LD_LIBRARY_PATH >> /home/ubuntu/watchdog.log\n",
    "watchdog.py >> /home/ubuntu/watchdog.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load extractPatches.py\n",
    "import cv2\n",
    "from cv2 import moments,HuMoments\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def create_footprint(cell_size=41):\n",
    "    center=(cell_size-1)/2.\n",
    "    footprint=np.ones([cell_size,cell_size])>1\n",
    "\n",
    "    for i in range(cell_size):\n",
    "        for j in range(cell_size):\n",
    "            footprint[i,j]=((i-center)**2+(j-center)**2)<=center**2\n",
    "            \n",
    "    ratio=sum(footprint.flatten())/(cell_size**2) # ratio of footprint area to square patch area\n",
    "    return cell_size,center,ratio,footprint\n",
    "\n",
    "def normalize_greyvals(ex):\n",
    "    ex=ex*mask\n",
    "    _flat=ex.flatten()\n",
    "    _m=np.mean(_flat)/ratio\n",
    "    _m2=np.mean(_flat**2)/ratio\n",
    "    #print(_m.shape,_m2.shape,_flat.shape)\n",
    "    if _m2>_m**2:\n",
    "        _std=np.sqrt(_m2-_m**2)\n",
    "    else:\n",
    "        _std=1\n",
    "        print('error in calc of _std',_m,_m2)\n",
    "    #print('normalize_greyvals: mean=',_m,'std=',_std)\n",
    "    ex_new=(ex-_m)/_std\n",
    "    return _m,_std,ex_new * mask\n",
    "\n",
    "def angle(ex):\n",
    "    rows,cols = ex.shape\n",
    "    M=moments(ex+mask)\n",
    "    #print(M['m00'],M['m10'],M['m01'])\n",
    "    x=M['m10']/M['m00']\n",
    "    y=M['m01']/M['m00']\n",
    "    nu20=(M['m20']/M['m00'])-x**2\n",
    "    nu02=(M['m02']/M['m00'])-y**2\n",
    "    nu11=(M['m11']/M['m00'])-y*x\n",
    "    ang_est=-np.arctan(2*nu11/(nu20-nu02))/np.pi+0.5\n",
    "\n",
    "    if ang_est>0.5:\n",
    "        ang_est-=1\n",
    "    ang180=(ang_est+(np.sign(nu11))/2)*90\n",
    "\n",
    "    if ang180>=180:\n",
    "        ang180-=360\n",
    "    if ang180<-180:\n",
    "        ang180+=360\n",
    "    return ang180\n",
    "\n",
    "def flipOrNot(ex):\n",
    "    rows,cols = ex.shape\n",
    "    M=moments(ex)\n",
    "    x=M['m10']/M['m00'] - cols/2.\n",
    "    y=M['m01']/M['m00'] - rows/2.\n",
    "    if abs(x)>abs(y):\n",
    "        return x<0\n",
    "    else:\n",
    "        return y<0\n",
    "\n",
    "\n",
    "def normalize_angle(ex):\n",
    "    rows,cols = ex.shape\n",
    "    ang=angle(ex)\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),-ang,1)\n",
    "    dst= cv2.warpAffine(ex,M,(cols,rows))*footprint*1\n",
    "    if flipOrNot(dst):\n",
    "        M180 = cv2.getRotationMatrix2D((cols/2,rows/2),180,1)\n",
    "        dst = cv2.warpAffine(dst,M180,(cols,rows))\n",
    "\n",
    "    return ang,dst*footprint*1.\n",
    "\n",
    "from astropy.convolution import MexicanHat2DKernel,convolve\n",
    "mexicanhat_2D_kernel = 10000*MexicanHat2DKernel(10)\n",
    "\n",
    "def find_threshold(image,percentile=0.9):\n",
    "    V=sorted(image.flatten())\n",
    "    l=len(V)\n",
    "    thr=V[int(l*percentile)] #consider only peaks in the top 5%\n",
    "    return thr\n",
    "\n",
    "def normalize(window,range=[0,1],dtype=np.float32):\n",
    "    _max=max(window.flatten())\n",
    "    _min=min(window.flatten())\n",
    "    return np.array((window-_min)/(_max-_min),dtype=dtype)\n",
    "\n",
    "from photutils.detection import find_peaks\n",
    "\n",
    "def extract_patches(_mean,Peaks):\n",
    "    markers=_mean*np.float32(0)\n",
    "    stamp=footprint*np.float32(0.2)\n",
    "\n",
    "    X=list(Peaks[\"x_peak\"])\n",
    "    Y=list(Peaks[\"y_peak\"])\n",
    "\n",
    "    extracted=[]\n",
    "    for i in range(len(X)):\n",
    "        corner_x=np.uint16(X[i]-center)\n",
    "        corner_y=np.uint16(Y[i]-center)\n",
    "\n",
    "        # ignore patches that extend outside of window\n",
    "        if(corner_x<0 or corner_y<0 or \\\n",
    "           corner_x+cell_size>markers.shape[1] or corner_y+cell_size>markers.shape[0]):\n",
    "            continue\n",
    "\n",
    "        # mark location of extracted patches\n",
    "        markers[corner_y:corner_y+cell_size,corner_x:corner_x+cell_size]=stamp\n",
    "        # extract patch\n",
    "        ex=np.array(_mean[corner_y:corner_y+cell_size,corner_x:corner_x+cell_size])\n",
    "        ex *= mask\n",
    "\n",
    "        #normalize patch interms of grey values and in terms of rotation\n",
    "        _m,_std,ex_grey_normed=normalize_greyvals(ex)\n",
    "        rot_angle1,ex_rotation_normed=normalize_angle(ex_grey_normed)\n",
    "        rot_angle2,ex_rotation_normed=normalize_angle(ex_rotation_normed)\n",
    "        extracted.append((_m,_std,rot_angle1+rot_angle2,ex_rotation_normed*mask))\n",
    "    return extracted,markers\n",
    "    \n",
    "def check_blank(window,min_std=10):\n",
    "    # find whether window mosly blank and should be ignored.\n",
    "    return np.std(window.flatten()) < min_std\n",
    "\n",
    "def preprocess(window):\n",
    "    _mean=normalize(window)\n",
    "    P=convolve(_mean,mexicanhat_2D_kernel)\n",
    "    thr=find_threshold(P,0.9)\n",
    "    Peaks=find_peaks(P,thr,footprint=footprint)\n",
    "    return _mean,P,Peaks\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    import argparse\n",
    "    from time import time\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"filestem\", type=str,\n",
    "                    help=\"Process <filestem>.pkl into <filestem>_extracted.pkl\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    infile = args.filestem+'.tif'\n",
    "    outfile= args.filestem+'_extracted.pkl'\n",
    "\n",
    "    window=cv2.imread(infile,cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "    if check_blank(window):\n",
    "        print('image',infile,'too blank, skipping')\n",
    "    else:\n",
    "        t0=time()\n",
    "        print('processing',infile,'into',outfile)\n",
    "        cell_size,center,ratio,footprint=create_footprint(cell_size=41)\n",
    "        mask=1.*footprint\n",
    "        _mean,P,Peaks=preprocess(window)    \n",
    "        print('found',len(Peaks),'patches')\n",
    "        extracted,markers=extract_patches(_mean,Peaks)\n",
    "\n",
    "        pickle.dump(extracted,open(outfile,'wb'))\n",
    "        print('finished in %5.1f seconds'%(time()-t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Create sequence of VQs.py\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from glob import glob\n",
    "from subprocess import Popen,PIPE\n",
    "from os import system\n",
    "from os.path import isfile\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.convolution import Gaussian2DKernel,convolve\n",
    "\n",
    "def run(command):\n",
    "    print('cmd=',command)\n",
    "    system(command)\n",
    "    \n",
    "def runPipe(command):\n",
    "    print('runPipe cmd=',command)\n",
    "    p=Popen(command.split(),stdout=PIPE,stderr=PIPE)\n",
    "    L=p.communicate()\n",
    "    stdout=L[0].decode(\"utf-8\").split('\\n')\n",
    "    stderr=L[1].decode(\"utf-8\").split('\\n')\n",
    "    return stdout,stderr\n",
    "\n",
    "def clock(message):\n",
    "    print('%8.1f \\t%s'%(time(),message))\n",
    "    time_log.append((time(),message))\n",
    "\n",
    "def printClock():\n",
    "    t=time_log[0][0]\n",
    "    for i in range(1,len(time_log)):\n",
    "        print('%8.1f \\t%s'%(time_log[i][0]-t,time_log[i][1]))\n",
    "        t=time_log[i][0]\n",
    "\n",
    "def list_s3_files(stack_directory):\n",
    "    stdout,stderr=runPipe(\"aws s3 ls %s/ \"%(stack_directory))\n",
    "    filenames=[]\n",
    "    for line in stdout:\n",
    "        parts=line.strip().split()\n",
    "        if len(parts)!=4:\n",
    "            continue\n",
    "        filenames.append(parts[-1])\n",
    "    return filenames\n",
    "\n",
    "def read_files(s3_dir,_delete=False):\n",
    "    s3files=list_s3_files(s3_dir)\n",
    "    for filename in s3files:\n",
    "        if not isfile(data_dir+'/'+filename):\n",
    "            run('aws s3 cp %s/%s %s'%(s3_dir,filename,data_dir))\n",
    "        D=fromfile(data_dir+'/'+filename,dtype=np.float16)\n",
    "        pics=D.reshape([-1,_size,_size])\n",
    "        if _delete:\n",
    "            run('rm %s/%s'%(data_dir,filename))\n",
    "        yield pics\n",
    "\n",
    "def data_stream(s3_dir='s3://mousebraindata-open/MD657/permuted'):\n",
    "    for pics in read_files(s3_dir):\n",
    "        j=0\n",
    "        for i in range(pics.shape[0]):\n",
    "            if j%1000==0:\n",
    "                print('\\r examples read=%10d'%j,end='')\n",
    "            j+=1    \n",
    "            yield pics[i,:,:]\n",
    "\n",
    "def calc_err(pic,gaussian = Gaussian2DKernel(1,x_size=7,y_size=7)):\n",
    "    factor=sum(gaussian)\n",
    "    P=convolve(pic,gaussian)/factor\n",
    "    error=sqrt(mean(abs(pic-P)))\n",
    "    sub=P[::2,::2]\n",
    "    return error,sub\n",
    "\n",
    "def plot_patches(data,h=40,w=15,_titles=[]):\n",
    "    figure(figsize=(w*2,h*2))\n",
    "    for i in range(h*w):\n",
    "        if i>=data.shape[0]:\n",
    "            break\n",
    "        subplot(h,w,i+1);\n",
    "        pic=np.array(data[i,:,:],dtype=np.float32)\n",
    "\n",
    "        fig=imshow(pic,cmap='gray')\n",
    "        if(len(_titles)>i):\n",
    "            plt.title(_titles[i])\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "def pack_pics(Reps):\n",
    "    size=Reps[0].shape[0]\n",
    "    _len=len(Reps)\n",
    "    Reps_mat=np.zeros([_len,size,size])\n",
    "    for i in range(_len):\n",
    "        Reps_mat[i,:,:]=Reps[i]\n",
    "    return Reps_mat\n",
    "\n",
    "def dist2(a,b):\n",
    "    diff=(a-b)**2\n",
    "    return sum(diff.flatten())\n",
    "\n",
    "def dist_hist(data):\n",
    "    D=[]\n",
    "    for i in range(1,data.shape[0]):\n",
    "        D.append(dist2(data[i,:,:],data[i-1,:,:]))\n",
    "        if i%1000==0:\n",
    "            print('\\r',i,end='')\n",
    "    hist(D,bins=100);\n",
    "\n",
    "def refineKmeans(data_stream,Reps,per_rep_sample=100,refinement_iter=3):\n",
    "    _shape=Reps[0].shape\n",
    "    new_Reps=[np.zeros(_shape) for r in Reps]\n",
    "    _area=_shape[0]*_shape[1]\n",
    "    Reps_count=[0.0 for r in Reps]\n",
    "    error=0\n",
    "    count=per_rep_sample*len(Reps)\n",
    "    i=0\n",
    "    for patch in data_stream: \n",
    "        dists=[dist2(patch,r) for r in Reps]\n",
    "        _argmin=argmin(dists)\n",
    "        _min=min(dists)\n",
    "        new_Reps[_argmin]+=patch\n",
    "        Reps_count[_argmin]+=1\n",
    "        error+=_min\n",
    "        i+=1\n",
    "        if i >= count:\n",
    "            break\n",
    "    error /= (count*_area)\n",
    "    final_Reps=[]\n",
    "    final_counts=[]\n",
    "    for i in range(len(new_Reps)):\n",
    "        if Reps_count[i]>refinement_iter:\n",
    "            final_Reps.append(new_Reps[i]/Reps_count[i])\n",
    "            final_counts.append(Reps_count[i])\n",
    "    return final_Reps,final_counts,error\n",
    "\n",
    "def Kmeans(data_stream,Reps=[],n=100,scale=550):\n",
    "    Reps,Statistics = Kmeanspp(data_stream,Reps,n,scale)\n",
    "    for i in range(5):\n",
    "        Reps,final_counts,error = refineKmeans(data_stream,Reps)\n",
    "        print('refine iteration %2d, error=%7.3f, n_Reps=%5d'%(i,error,len(Reps)))\n",
    "    return Reps,final_counts\n",
    "\n",
    "def Kmeanspp(data_stream,Reps=[],n=100,scale=550):\n",
    "    if len(Reps)==0:\n",
    "        Reps=[next(data_stream)]\n",
    "\n",
    "    Statistics=[]\n",
    "    j=len(Reps)\n",
    "    for patch in data_stream: \n",
    "        _min=100000\n",
    "        for r in Reps:\n",
    "            _min=min(_min,dist2(patch,r))\n",
    "        Prob=_min/scale\n",
    "        print('\\r','i=%10d,  #reps=%10d  Prob=%8.6f'%(i,len(Reps),Prob),end='')\n",
    "        Statistics.append((i,len(Reps),_min))\n",
    "        if np.random.rand()<Prob:\n",
    "            Reps.append(patch)\n",
    "            j+=1\n",
    "        if j>=n:\n",
    "            break\n",
    "    return Reps,Statistics\n",
    "\n",
    "def plot_statistics(Statistics,alpha=0.05,_start=10): \n",
    "    N=[x[1] for x in Statistics]\n",
    "    d=[x[2] for x in Statistics]\n",
    "\n",
    "    s=mean(d[:_start])\n",
    "    smoothed=[s]*_start\n",
    "    for x in d[_start:]:\n",
    "        s=(1-alpha)*s + alpha*x\n",
    "        smoothed.append(s)\n",
    "    loglog(N[_start:],smoothed[_start:])\n",
    "    xlabel('N')\n",
    "    ylabel('smoothed distance')\n",
    "    grid(which='both')\n",
    "\n",
    "\n",
    "def filtered_images(s3_dir='s3://mousebraindata-open/MD657/permuted',reduce_res=True,smooth_threshold=0.4):\n",
    "    for pic in data_stream(s3_dir):\n",
    "        err,sub=calc_err(pic)\n",
    "        if err>smooth_threshold:\n",
    "            continue\n",
    "        if reduce_res:\n",
    "            yield sub\n",
    "        else:\n",
    "            yield pic\n",
    "\n",
    "\n",
    "gen=filtered_images(smooth_threshold=0.35,reduce_res=False)\n",
    "Reps,final_count=Kmeans(gen,n=10)\n",
    "plot_patches(pack_pics(Reps),_titles=['%4d'%x for x in final_count])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load diffusion_maps.py\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from glob import glob\n",
    "from subprocess import Popen,PIPE\n",
    "from os import system\n",
    "from os.path import isfile\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.convolution import Gaussian2DKernel,convolve\n",
    "\n",
    "# ## Calculate laplacian random-walk matrix\n",
    "# \n",
    "# The matrix corresponds to a simple random walk on the individual examples  where the location of each example is replaced by the location of the corresponding representative. \n",
    "# \n",
    "# We use [pydiffmap](https://pydiffmap.readthedocs.io/en/master/)\n",
    "\n",
    "# In[84]:\n",
    "\n",
    "\n",
    "L=len(new_Reps)\n",
    "new_Reps[0].shape\n",
    "data1D=np.concatenate([x.reshape([1,441]) for x in new_Reps])\n",
    "data1D.shape\n",
    "\n",
    "\n",
    "# In[85]:\n",
    "\n",
    "\n",
    "dists=[]\n",
    "for i in range(L):\n",
    "    for j in range(i):\n",
    "        dists.append(dist2(new_Reps[i],new_Reps[j]))\n",
    "hist(dists);\n",
    "title('distances between centroids')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "get_ipython().system('sudo pip3 install pydiffmap')\n",
    "\n",
    "\n",
    "# In[86]:\n",
    "\n",
    "\n",
    "data1D.shape,len(Reps_count)\n",
    "\n",
    "\n",
    "# In[87]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pydiffmap import diffusion_map as dm\n",
    "# initialize Diffusion map object.\n",
    "neighbor_params = {'n_jobs': -1, 'algorithm': 'ball_tree'}\n",
    "\n",
    "mydmap = dm.DiffusionMap(n_evecs=50, k=20, epsilon=100.0, alpha=1.0, neighbor_params=neighbor_params)\n",
    "# fit to data and return the diffusion map.\n",
    "dmap = mydmap.fit_transform(data1D,weights=Reps_count)\n",
    "\n",
    "#%pylab inline\n",
    "pylab.scatter(dmap[:,0],dmap[:,1]);\n",
    "\n",
    "\n",
    "# In[88]:\n",
    "\n",
    "\n",
    "hist(Reps_count);\n",
    "title('no. of examples per partition')\n",
    "\n",
    "\n",
    "# In[89]:\n",
    "\n",
    "\n",
    "image_size=np.array(new_Reps[0].shape)\n",
    "canvas_size=np.array([1000,1000])\n",
    "_minx=min(dmap[:,0])\n",
    "_maxx=max(dmap[:,0])\n",
    "_miny=min(dmap[:,1])\n",
    "_maxy=max(dmap[:,1])\n",
    "shift_x = -_minx\n",
    "scale_x = canvas_size[0]/(_maxx - _minx)\n",
    "shift_y = -_miny\n",
    "scale_y = canvas_size[1]/(_maxy - _miny)\n",
    "\n",
    "x=[int((_x+shift_x)*scale_x) for _x in dmap[:,0]]\n",
    "y=[int((_y+shift_y)*scale_y) for _y in dmap[:,1]]\n",
    "\n",
    "canvas=2*np.ones(canvas_size+image_size)\n",
    "for i in range(len(new_Reps)):\n",
    "    if(Reps_count[i]>30):\n",
    "        canvas[x[i]:x[i]+image_size[0],y[i]:y[i]+image_size[1]]=new_Reps[i]\n",
    "\n",
    "\n",
    "# In[90]:\n",
    "\n",
    "\n",
    "figure(figsize=[40,40])\n",
    "imshow(canvas,cmap='gray')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "max(new_Reps[0].flatten())\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from pydiffmap.visualization import embedding_plot, data_plot\n",
    "\n",
    "embedding_plot(mydmap, scatter_kwargs = {'c': dmap[:,0], 'cmap': 'Spectral'})\n",
    "data_plot(mydmap, dim=3, scatter_kwargs = {'cmap': 'Spectral'})\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "A=np.zeros([L,L])\n",
    "for i in range(L):\n",
    "    for j in range(L):\n",
    "        w=exp(-dist2(new_Reps[i],new_Reps[j])/sigma2)\n",
    "        A[i,j]=w * Reps_count[i]*Reps_count[j]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "D=sum(A,axis=0)\n",
    "D2=diag(1/sqrt(D))\n",
    "\n",
    "NA=np.dot(D2,np.dot(A,D2))\n",
    "\n",
    "w,v = np.linalg.eig(NA)\n",
    "hist(NA.flatten(),bins=100);\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "i=0\n",
    "print('eig no %3d eigval=%5.3f'%(i,w[i]))\n",
    "sorted_v=sort(v[i,:])\n",
    "order=argsort(v[i,:])\n",
    "plot_patches(Reps_mat[order],h=10,w=10,_titles=['c_%1d=%6.3f'%(i,x) for x in sorted_v])\n",
    "\n",
    "for i in range(L):\n",
    "    for j in range(L):\n",
    "        print(' %0.1f'%v[i,j],end='')\n",
    "    print('')\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "plot(w[:10])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "scatter(v[1,:],v[2,:])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "v[2,:]\n",
    "\n",
    "\n",
    "\n",
    "def run(command):\n",
    "    print('cmd=',command)\n",
    "    system(command)\n",
    "    \n",
    "def runPipe(command):\n",
    "    print('runPipe cmd=',command)\n",
    "    p=Popen(command.split(),stdout=PIPE,stderr=PIPE)\n",
    "    L=p.communicate()\n",
    "    stdout=L[0].decode(\"utf-8\").split('\\n')\n",
    "    stderr=L[1].decode(\"utf-8\").split('\\n')\n",
    "    return stdout,stderr\n",
    "\n",
    "def clock(message):\n",
    "    print('%8.1f \\t%s'%(time(),message))\n",
    "    time_log.append((time(),message))\n",
    "\n",
    "def printClock():\n",
    "    t=time_log[0][0]\n",
    "    for i in range(1,len(time_log)):\n",
    "        print('%8.1f \\t%s'%(time_log[i][0]-t,time_log[i][1]))\n",
    "        t=time_log[i][0]\n",
    "\n",
    "def list_s3_files(stack_directory):\n",
    "    stdout,stderr=runPipe(\"aws s3 ls %s/ \"%(stack_directory))\n",
    "    filenames=[]\n",
    "    for line in stdout:\n",
    "        parts=line.strip().split()\n",
    "        if len(parts)!=4:\n",
    "            continue\n",
    "        filenames.append(parts[-1])\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "s3_dir='s3://mousebraindata-open/MD657/permuted'\n",
    "permuted_files=list_s3_files(s3_dir)\n",
    "permuted_files[:10]\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "get_ipython().system('touch $data_dir/tmp')\n",
    "get_ipython().system('ls $data_dir')\n",
    "isfile(data_dir+'/tmp')\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "def read_files(s3_dir,_delete=False):\n",
    "    s3files=list_s3_files(s3_dir)\n",
    "    for filename in s3files:\n",
    "        if not isfile(data_dir+'/'+filename):\n",
    "            run('aws s3 cp %s/%s %s'%(s3_dir,filename,data_dir))\n",
    "        D=fromfile(data_dir+'/'+filename,dtype=np.float16)\n",
    "        pics=D.reshape([-1,_size,_size])\n",
    "        if _delete:\n",
    "            run('rm %s/%s'%(data_dir,filename))\n",
    "        yield pics\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "def data_stream():\n",
    "    for pics in read_files('s3://mousebraindata-open/MD657/permuted'):\n",
    "        for i in range(pics.shape[0]):\n",
    "            yield pics[i,:,:]\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "get_ipython().system('ls -lrt ../../data')\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "pics_list=[]\n",
    "i=0\n",
    "for pic in data_stream():\n",
    "    pics_list.append(np.array(pic,dtype=np.float32))\n",
    "    i+=1\n",
    "    if i>=300000:\n",
    "        break\n",
    "\n",
    "factor=sum(gaussian = Gaussian2DKernel(1,x_size=7,y_size=7))\n",
    "print('factor=',factor)\n",
    "def calc_err(pic):\n",
    "    P=convolve(pic,gaussian)/factor\n",
    "    error=sqrt(mean(abs(pic-P)))\n",
    "    sub=P[::2,::2]\n",
    "    return error,sub\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "def plot_patches(data,h=40,w=15,_titles=[]):\n",
    "    figure(figsize=(w*2,h*2))\n",
    "    for i in range(h*w):\n",
    "        if i>=data.shape[0]:\n",
    "            break\n",
    "        subplot(h,w,i+1);\n",
    "        pic=data[i,:,:]\n",
    "        #P=convolve(pic,gaussian)/factor\n",
    "\n",
    "        fig=imshow(pic,cmap='gray')\n",
    "        if(len(_titles)>i):\n",
    "            plt.title(_titles[i])\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "#plot_patches(scombined,h=2,_titles=[str(i) for i in range(scombined.shape[0])])\n",
    "\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "\n",
    "def pack_pics(Reps):\n",
    "    size=Reps[0].shape[0]\n",
    "    _len=len(Reps)\n",
    "    Reps_mat=np.zeros([_len,size,size])\n",
    "    for i in range(_len):\n",
    "        Reps_mat[i,:,:]=Reps[i]\n",
    "    return Reps_mat\n",
    "\n",
    "\n",
    "# In[68]:\n",
    "\n",
    "\n",
    "pics=pack_pics(pics_list)\n",
    "\n",
    "\n",
    "# In[69]:\n",
    "\n",
    "\n",
    "plot_patches(pics)\n",
    "\n",
    "\n",
    "# In[70]:\n",
    "\n",
    "\n",
    "shift=1000\n",
    "selected=[]\n",
    "errors=[]\n",
    "i=0; j=0;\n",
    "while j < 100:\n",
    "    pic=pics[i+shift,:,:]\n",
    "    i+=1\n",
    "    error,sub=calc_err(pic)\n",
    "    if error<0.4:\n",
    "        continue\n",
    "    j+=1\n",
    "    selected.append(pic)\n",
    "    errors.append('%4.2f'%error)\n",
    "plot_patches(pack_pics(selected),h=10,w=10,_titles=errors)\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "\n",
    "#collect images that are pretty smooth\n",
    "# reduce resolution by a factor of 2\n",
    "low_err=[]\n",
    "for i in range(pics.shape[0]):\n",
    "    pic=pics[i,:,:]\n",
    "    error,sub=calc_err(pic)\n",
    "    if error<0.4:\n",
    "        low_err.append(sub)\n",
    "        j=len(low_err)\n",
    "        if j%1000==0:\n",
    "            print('\\r',i,j,end='')\n",
    "\n",
    "lcombined=np.stack(low_err)\n",
    "lcombined.shape\n",
    "\n",
    "\n",
    "# In[72]:\n",
    "\n",
    "\n",
    "lcombined.shape\n",
    "\n",
    "\n",
    "# In[73]:\n",
    "\n",
    "\n",
    "def dist2(a,b):\n",
    "    diff=(a-b)**2\n",
    "    return sum(diff.flatten())\n",
    "\n",
    "\n",
    "# In[74]:\n",
    "\n",
    "\n",
    "D=[]\n",
    "for i in range(1,lcombined.shape[0]):\n",
    "    D.append(dist2(lcombined[i,:,:],lcombined[i-1,:,:]))\n",
    "    if i%1000==0:\n",
    "        print('\\r',i,end='')\n",
    "\n",
    "\n",
    "# In[75]:\n",
    "\n",
    "\n",
    "hist(D,bins=100);\n",
    "\n",
    "\n",
    "# In[76]:\n",
    "\n",
    "\n",
    "max(D)\n",
    "\n",
    "\n",
    "# In[77]:\n",
    "\n",
    "\n",
    "def refineKmeans(data,Reps):\n",
    "    new_Reps=[np.zeros(Reps[0].shape) for r in Reps]\n",
    "    Reps_count=[0.0 for r in Reps]\n",
    "    error=0\n",
    "    for i in range(data.shape[0]): \n",
    "        patch=data[i,:,:]\n",
    "        dists=[dist2(patch,r) for r in Reps]\n",
    "        _argmin=argmin(dists)\n",
    "        _min=min(dists)\n",
    "        new_Reps[_argmin]+=patch\n",
    "        Reps_count[_argmin]+=1\n",
    "        error+=_min\n",
    "    error /= data.shape[0]\n",
    "    final_Reps=[]\n",
    "    final_counts=[]\n",
    "    for i in range(len(new_Reps)):\n",
    "        if Reps_count[i]>5:\n",
    "            final_Reps.append(new_Reps[i]/Reps_count[i])\n",
    "            final_counts.append(Reps_count[i])\n",
    "    return final_Reps,final_counts,error\n",
    "\n",
    "\n",
    "# In[78]:\n",
    "\n",
    "\n",
    "def Kmeans(data,n=100,scale=550):\n",
    "    Reps,Statistics = Kmeanspp(data,n,scale)\n",
    "    for i in range(5):\n",
    "        Reps,error = refineKmeans(data,Reps)\n",
    "        print('refine iteration %2d, error=%7.3f'%(i,error))\n",
    "\n",
    "\n",
    "# In[79]:\n",
    "\n",
    "\n",
    "def Kmeanspp(data,n=100,scale=550):\n",
    "    Reps=[data[0,:,:]]\n",
    "\n",
    "    Statistics=[]\n",
    "    j=1\n",
    "    for i in range(1,data.shape[0]): \n",
    "        _min=100000\n",
    "        patch=data[i,:,:]\n",
    "        for r in Reps:\n",
    "            _min=min(_min,dist2(patch,r))\n",
    "        Prob=_min/scale\n",
    "        print('\\r','i=%10d,  #reps=%10d  Prob=%8.6f'%(i,len(Reps),Prob),end='')\n",
    "        Statistics.append((i,len(Reps),_min))\n",
    "        if np.random.rand()<Prob:\n",
    "            Reps.append(patch)\n",
    "            j+=1\n",
    "        if j>=n:\n",
    "            break\n",
    "    return Reps,Statistics\n",
    "\n",
    "\n",
    "# In[80]:\n",
    "\n",
    "\n",
    "def plot_statistics(Statistics,alpha=0.05,_start=10): \n",
    "    N=[x[1] for x in Statistics]\n",
    "    d=[x[2] for x in Statistics]\n",
    "\n",
    "    s=mean(d[:_start])\n",
    "    smoothed=[s]*_start\n",
    "    for x in d[_start:]:\n",
    "        s=(1-alpha)*s + alpha*x\n",
    "        smoothed.append(s)\n",
    "    loglog(N[_start:],smoothed[_start:])\n",
    "    xlabel('N')\n",
    "    ylabel('smoothed distance')\n",
    "    grid(which='both')\n",
    "\n",
    "\n",
    "# In[81]:\n",
    "\n",
    "\n",
    "N=300\n",
    "\n",
    "Reps, Statistics = Kmeanspp(lcombined,n=N)\n",
    "Reps_mat = pack_pics(Reps)\n",
    "plot_patches(Reps_mat,h=5,w=10)\n",
    "\n",
    "\n",
    "# In[82]:\n",
    "\n",
    "\n",
    "Reps_mat.shape\n",
    "\n",
    "\n",
    "# In[83]:\n",
    "\n",
    "\n",
    "for i in range(1,4):\n",
    "    new_Reps,Reps_count,error = refineKmeans(lcombined[i*10000:(i+1)*10000,:,:],Reps)\n",
    "    print(i,error,len(Reps_count))\n",
    "    Reps_mat = pack_pics(new_Reps)\n",
    "    plot_patches(Reps_mat,h=1,w=10,_titles=['%4d'%x for x in Reps_count])\n",
    "    Reps=new_Reps\n",
    "plot_patches(Reps_mat,h=10,w=10,_titles=['final_%4d'%x for x in Reps_count])\n",
    "\n",
    "\n",
    "# ## Calculate laplacian random-walk matrix\n",
    "# \n",
    "# The matrix corresponds to a simple random walk on the individual examples  where the location of each example is replaced by the location of the corresponding representative. \n",
    "# \n",
    "# We use [pydiffmap](https://pydiffmap.readthedocs.io/en/master/)\n",
    "\n",
    "# In[84]:\n",
    "\n",
    "\n",
    "L=len(new_Reps)\n",
    "new_Reps[0].shape\n",
    "data1D=np.concatenate([x.reshape([1,441]) for x in new_Reps])\n",
    "data1D.shape\n",
    "\n",
    "\n",
    "# In[85]:\n",
    "\n",
    "\n",
    "dists=[]\n",
    "for i in range(L):\n",
    "    for j in range(i):\n",
    "        dists.append(dist2(new_Reps[i],new_Reps[j]))\n",
    "hist(dists);\n",
    "title('distances between centroids')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "get_ipython().system('sudo pip3 install pydiffmap')\n",
    "\n",
    "\n",
    "# In[86]:\n",
    "\n",
    "\n",
    "data1D.shape,len(Reps_count)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    _size=41\n",
    "    data_dir=\"../../data\"\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
