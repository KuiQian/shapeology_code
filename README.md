## Generating patches
#### process_file.py
```
A script that takes in an S3 directory breaks it into tiles and extracts
normalized patches from these tiles.

positional arguments:
  s3location  path to the s3 directory with the lossless images
  stem        the file name stem
  yaml        Path to Yaml file with parameters

```
#### run_job.py
Wrapper around extract Patches
```
positional arguments:
  stem        the file to be processed
  yaml        Path to Yaml file with parameters
```

#### extractPatches.py
Extract patches from a single tile
```
positional arguments:
  filestem    Process <filestem>.tif into <filestem>_extracted.pkl
  yaml        Path to Yaml file with parameters
```

#### patch_normalizer.py
Normalize the patches - called by extractPatches.py

Class `normalizer`

Not called as a script

## Analysis of patches

#### post-processing.py 

```Process the patch files generated by extractPatches.py and prepare
them for analysis by Kmeans and diffusion-maps```

#### CreateVQs.py

a script currently implemented as a notebook: CreateSequenceVQ.ipynb 

#### diffusion_maps.py


#### label_patch.py



## Managing multiple ec2 instances
#### Controller.py
The controller runs a given script on a set of files on S3. The application is intended to run on a set of ec2 instances in parallel.
Lock files are used to insure that each file is processed exactly once.

This python file will be rewritten in a simpler way using datajoin

#### watchdxog.sh
Command put in cron job to run watchdog.py

#### watchdog.py
script to check whether controller is running and, if not, restart it.

## Notebooks

#### Process_Extracted _Patches.ipynb
Partition the patches by size, permute, and store in binary files.

#### CreateSequenceVQ.ipynb
Perform Kmeans++, Kmeans and diffusion-based analysis

label_patch.py
runAll.py
